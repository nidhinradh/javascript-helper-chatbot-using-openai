{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63add0e",
   "metadata": {},
   "source": [
    "This is a Jupyter Notebook. When you execute code within the notebook, the results appear beneath the code. To execute a code chunk, place your cursor on the cell and press Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7fe1e",
   "metadata": {},
   "source": [
    "In this tutorial we will be building a Javascript helper chatbot using OpenAIs GPT-3 engine. This chatbot will be capable of answering all your Javascript related queries.\n",
    "\n",
    "Generative Pre-trained Transformer 3 (GPT-3) is a new language model created by OpenAI that is able to generate written text of such quality that is often difficult to differentiate from text written by a human.\n",
    "\n",
    "You need an OpenAI GPT-3 API key with codex engine access for testing out the code in this blog. At the time I’m writing this, OpenAI is running a beta program for GPT-3, and you can [signup for the beta program](https://beta.openai.com/) here. After signipng up you need to apply for Codex access [here](http://beta.openai.com/codex-waitlist). \n",
    "Read the complete get started guide [here](https://openai.com/blog/openai-api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5bb0e",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c745806",
   "metadata": {},
   "source": [
    "In December 2015, Elon Musk with other investors announced the formation of OpenAI. The organization stated they would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public.\n",
    "In 2019, OpenAI transitioned from non-profit to for-profit organization. The company distributed equity to its employees and partnered with Microsoft Corporation, who announced an investment package of US$1 billion into the company. OpenAI then announced its intention to commercially license its technologies, with Microsoft as its preferred partner.\n",
    "In June 2020, OpenAI announced GPT-3, a language model trained on trillions of words from the Internet. Today we will be using this model in our helper chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47304864",
   "metadata": {},
   "source": [
    "### OpenAI Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9ed6f",
   "metadata": {},
   "source": [
    "When you sign up for OpenAI beta API, you get a $18 credit and an API key so you can immediately start to work with the API. Before starting, it is important to understand the different engines provided by the API — as they have different capabilities, response times and costs associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b7169",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><th>MODELS</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>Base series</td><td>A set of GPT-3 models that can understand and generate natural language</td></tr>\n",
    "<tr><td>Instruct series(Beta)</td><td>A set of specialized models that are similar to the base series, but better at following your instructions</td></tr>\n",
    "<tr><td>Codex series(Private Beta)</td><td>A set of models that can understand and generate code, including translating natural language to code</td></tr>\n",
    "<tr><td>Content filter</td><td>A fine-tuned model that can detect whether text may be sensitive or unsafe</td></tr>\n",
    "</table>\n",
    "\n",
    "Open AI currently offer two Codex models:\n",
    "\n",
    "<table>\n",
    "<tr><th>MODELS</th><th>STRENGTH</th><th>REQUEST LENGTH</th></tr>\n",
    "<tr><td>davinci-codex</td><td>Most capable Codex model. Particularly good at translating natural language to code.</td><td>Up to 4,096 tokens(double the usual limit)</td></tr>\n",
    "<tr><td>cushman-codex</td><td>Almost as capable as Davinci Codex, but slightly faster. This speed advantage may make it preferable for real-time applications.</td><td>Up to 2,048 tokens</td></tr>\n",
    "</table>\n",
    "\n",
    "In our example we will be using davinci-codex as it is the most effiecient model that OpenAI has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2218663",
   "metadata": {},
   "source": [
    "### Install OpenAI in your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36e6d1",
   "metadata": {},
   "source": [
    "Run the following command from your terminal to install openai in your system. Installing it in a seperate virtual environment is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713828a",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112c9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21f871",
   "metadata": {},
   "source": [
    "Get your OpenAI API key from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99303f",
   "metadata": {},
   "source": [
    "We will define custom function that accepts a prompt string and returns response message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8626c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    return openai.Completion.create(\n",
    "      engine=\"davinci-codex\",\n",
    "      prompt=prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=180,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.5,\n",
    "      presence_penalty=0.0,\n",
    "      stop=[\"Me:\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd272c4",
   "metadata": {},
   "source": [
    "The parameters used in the request body are as follows,\n",
    "<table>\n",
    "<tr><th>Parameter</th><th>Description</th></tr>\n",
    "<tr><td>engine</td><td>The ID of the engine to use for this request</td></tr>\n",
    "<tr><td>prompt</td><td>The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.</td></tr>\n",
    "<tr><td>temperature</td><td>What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.</td></tr>\n",
    "<tr><td>max_tokens</td><td>The maximum number of tokens to generate in the completion.</td></tr>\n",
    "<tr><td>top_p</td><td>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</td></tr>\n",
    "<tr><td>frequency_penalty</td><td>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</td></tr>\n",
    "<tr><td>presence_penalty</td><td>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</td></tr>\n",
    "<tr><td>stop</td><td>Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78396c",
   "metadata": {},
   "source": [
    "Now we will define a loop that calls the get_response() method and prints the response after each user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da74d85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type exit for quitting the chatbot\n",
      "Type you message: Hello\n",
      "\u001b[A                             \u001b[A\n",
      "Me: Hello\n",
      "Helper Bot:  Hello\n",
      "Type you message: What is the use of await() method in Javascript?\n",
      "\u001b[A                             \u001b[A\n",
      "Me: What is the use of await() method in Javascript?\n",
      "Helper Bot:   await is a keyword in javascript which is used to call asynchronous functions.\n",
      "Type you message: Give me an example for promise in javascript?\n",
      "\u001b[A                             \u001b[A\n",
      "Me: Give me an example for promise in javascript?\n",
      "Helper Bot:    var promise = new Promise(function(resolve, reject) {\n",
      "    // do a thing, possibly async, then…\n",
      "    if (/* success */) {\n",
      "        resolve(/* value */);\n",
      "    } else {\n",
      "        reject(/* error */);\n",
      "    }\n",
      "});\n",
      "Type you message: What is the difference between resolve() and reject()?\n",
      "\u001b[A                             \u001b[A\n",
      "Me: What is the difference between resolve() and reject()?\n",
      "Helper Bot:     resolve() is used to resolve a promise with a value.\n",
      "    reject() is used to reject a promise with an error.\n",
      "Type you message: exit\n",
      "\u001b[A                             \u001b[A\n",
      "Me: exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Type exit for quitting the chatbot\")\n",
    "last_messages = [\"Helper Bot: Hai\"]\n",
    "while True:\n",
    "    user_input = input(\"Type you message: \")\n",
    "    print (\"\\033[A                             \\033[A\")\n",
    "    print(\"Me: \"+user_input)\n",
    "    if(user_input == \"exit\"):\n",
    "        break;\n",
    "    last_messages.append(\"Me: \"+user_input)\n",
    "    prompt = \"\\n\".join(last_messages[-3 if len(last_messages) > 3 else len(last_messages)-1:])\n",
    "    bot_response = get_response(prompt+\"\\nHelper Bot:\")[\"choices\"][0][\"text\"]\n",
    "    last_messages.append(\"Helper Bot: \"+bot_response)\n",
    "    print(last_messages[-1].rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55933b",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023dd16",
   "metadata": {},
   "source": [
    "Now we have built a Javascript helper chatbot using OpenAIs davicni-codex engine. You can refer to [OpenAI's official documentation](https://beta.openai.com/docs/introduction/overview) and experiment with the above code by changing different parameters.\n",
    "I love your feedback, please let me know what you think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a6738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
